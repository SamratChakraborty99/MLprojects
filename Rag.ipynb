{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mistralai.models.chat_completion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistralai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralClient\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmistralai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_completion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatMessage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mistralai.models.chat_completion'"
     ]
    }
   ],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "import PyPDF2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "class AnnualReportRAG:\n",
    "    def __init__(self, mistral_api_key):\n",
    "        \"\"\"Initialize the RAG system with Mistral API key.\"\"\"\n",
    "        # Initialize Mistral client for text generation\n",
    "        self.mistral_client = MistralClient(api_key=mistral_api_key)\n",
    "        \n",
    "        # Initialize embedding model (using a free alternative to Mistral embeddings)\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        # Initialize vector store as None (will be created when processing document)\n",
    "        self.vector_store = None\n",
    "        \n",
    "    def load_pdf(self, pdf_path):\n",
    "        \"\"\"Load and extract text from PDF file.\"\"\"\n",
    "        pdf_text = \"\"\n",
    "        # Open PDF file in binary read mode\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            # Create PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            # Iterate through each page and extract text\n",
    "            for page in pdf_reader.pages:\n",
    "                pdf_text += page.extract_text()\n",
    "        return pdf_text\n",
    "    \n",
    "    def process_document(self, text):\n",
    "        \"\"\"Split the document into chunks and create embeddings.\"\"\"\n",
    "        # Initialize text splitter with specific parameters\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,  # Number of characters per chunk\n",
    "            chunk_overlap=200,  # Number of overlapping characters between chunks\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        # Split text into manageable chunks\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        # Create FAISS vector store from text chunks\n",
    "        self.vector_store = FAISS.from_texts(\n",
    "            texts=chunks,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "    \n",
    "    def get_relevant_context(self, question, k=3):\n",
    "        \"\"\"Retrieve relevant context for the question.\"\"\"\n",
    "        if self.vector_store is None:\n",
    "            raise ValueError(\"Please process a document first!\")\n",
    "            \n",
    "        # Search for similar documents\n",
    "        docs = self.vector_store.similarity_search(question, k=k)\n",
    "        \n",
    "        # Combine all relevant contexts\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        return context, docs\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"Query the RAG system using Mistral AI.\"\"\"\n",
    "        # Get relevant context\n",
    "        context, source_docs = self.get_relevant_context(question)\n",
    "        \n",
    "        # Create prompt with context\n",
    "        system_prompt = \"\"\"You are a helpful AI assistant analyzing company annual reports. \n",
    "        Use the provided context to answer questions accurately. \n",
    "        If the information cannot be found in the context, say so.\"\"\"\n",
    "        \n",
    "        # Create messages for Mistral chat completion\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=system_prompt),\n",
    "            ChatMessage(role=\"user\", content=f\"\"\"Context: {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Please answer the question based on the context provided.\"\"\")\n",
    "        ]\n",
    "        \n",
    "        # Get response from Mistral\n",
    "        response = self.mistral_client.chat(\n",
    "            model=\"mistral-large-2402\",\n",
    "            messages=messages,\n",
    "            temperature=0.1,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.messages[0].content,\n",
    "            \"source_documents\": source_docs\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Initialize RAG system with Mistral API key\n",
    "    rag = AnnualReportRAG(os.getenv(\"Mistral_API_KEY\"))\n",
    "    \n",
    "    # Example usage\n",
    "    pdf_text = rag.load_pdf(\"RIL-Integrated-Annual-Report-2022-23.pdf\")\n",
    "    rag.process_document(pdf_text)\n",
    "    \n",
    "    # Example queries\n",
    "    questions = [\n",
    "        \"What was the company's revenue in the last fiscal year?\",\n",
    "        \"What are the main risk factors mentioned in the report?\",\n",
    "        \"What is the company's strategy for growth?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = rag.query(question)\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(\"\\nSources:\")\n",
    "        for doc in result['source_documents']:\n",
    "            print(f\"- {doc.page_content[:200]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
